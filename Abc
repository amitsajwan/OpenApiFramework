import json
from langchain.schema.runnable import RunnablePassthrough
from langchain.chat_models import AzureChatOpenAI
from langchain.prompts import ChatPromptTemplate

class LLMSequenceGenerator:
    def __init__(self, azure_endpoint, azure_key, deployment_name):
        self.llm = AzureChatOpenAI(
            openai_api_base=azure_endpoint,
            openai_api_version="2023-07-01-preview",
            deployment_name=deployment_name,
            openai_api_key=azure_key,
            temperature=0.3
        )

    def generate_sequence(self, api_map):
        """
        Determines the correct execution sequence of APIs based on dependencies.
        Ensures POST and GET are never lost.
        :param api_map: Dictionary of API details (method + path).
        :return: Ordered list of API execution paths.
        """
        api_list = [
            {"method": details["method"], "path": details["path"]}
            for details in api_map.values()
        ]

        api_list_str = json.dumps(api_list, indent=2)  # Properly formatted JSON

        print(f"üîπ Sent to LLM:\n{api_list_str}")  # Debugging log

        prompt_template = ChatPromptTemplate.from_messages([
            ("system", "You are an expert API tester. Your task is to return a JSON array of APIs in the correct order."),
            ("human", 
             """Given these APIs in JSON format, return them in the correct execution order.
             - **NEVER exclude any API** from the response.
             - The order should follow API dependencies:
               1Ô∏è‚É£ **POST first** (creates resource)
               2Ô∏è‚É£ **GET next** (retrieves resource)
               3Ô∏è‚É£ **PUT after GET** (updates resource)
               4Ô∏è‚É£ **DELETE last** (removes resource)
               5Ô∏è‚É£ **Independent GETs can be at the end.**

             Ensure that **POST and GET are always included** in the output.

             APIs:\n{api_list}

             **Return JSON only, no explanations.**""")
        ])

        chain = (
            RunnablePassthrough.assign(api_list=lambda _: api_list_str)
            | prompt_template
            | self.llm
        )

        response = chain.invoke({})

        try:
            ordered_apis = json.loads(response.content)  # Expecting JSON array
            
            # Check if POST and GET exist; if not, retry
            post_exists = any(api["method"] == "POST" for api in ordered_apis)
            get_exists = any(api["method"] == "GET" for api in ordered_apis)

            if not post_exists or not get_exists:
                print(f"‚ö†Ô∏è POST or GET missing, falling back to manual sorting!")
                ordered_apis = sorted(api_list, key=lambda x: ["POST", "GET", "PUT", "DELETE"].index(x["method"]))

        except:
            print(f"‚ö†Ô∏è LLM Response (Invalid JSON): {response.content}")
            ordered_apis = sorted(api_list, key=lambda x: ["POST", "GET", "PUT", "DELETE"].index(x["method"]))

        print(f"‚úÖ Ordered APIs:\n{json.dumps(ordered_apis, indent=2)}")  # Debugging output
        return ordered_apis
